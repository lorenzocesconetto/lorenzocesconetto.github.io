[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lorenzo Cesconetto",
    "section": "",
    "text": "I’m a software engineer based in Brazil. I have a wide variety of experiences, from Backend and DevOps to Data Science, Machine Learning (ML) and Artificial Intelligence (AI).\nI am passionate about the intersection of Computer Science, Mathematics and AI.\nI am particularly attracted to NLP applications."
  },
  {
    "objectID": "about.html#who-i-am",
    "href": "about.html#who-i-am",
    "title": "Lorenzo Cesconetto",
    "section": "",
    "text": "I’m a software engineer based in Brazil. I have a wide variety of experiences, from Backend and DevOps to Data Science, Machine Learning (ML) and Artificial Intelligence (AI).\nI am passionate about the intersection of Computer Science, Mathematics and AI.\nI am particularly attracted to NLP applications."
  },
  {
    "objectID": "about.html#why-i-started-this-blog",
    "href": "about.html#why-i-started-this-blog",
    "title": "Lorenzo Cesconetto",
    "section": "Why I started this blog",
    "text": "Why I started this blog\nI enjoy diving very deep when learning new things. I can only truly understand a new concept once I fully understand its inner workings.\nI believe there are other like-minded people around the world. If you’re one of them, hopefully you’ll find my content insightful.\nMost AI content on the internet is rather superficial, so I decided to share my way of seeing the world.\nBesides that:\n\nConnect with other like-minded folks\nDocument my learnings\nForce myself to write things down"
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Lorenzo Cesconetto",
    "section": "",
    "text": "Todo\n\nCreate first post (Distributed Training intro)\n\n\n\nOptimizations for later:\n\ni18n\nEmail subscription\n\n\n\nDone:\n\nTable listing of posts\nOpen links on a new tab\nDark/Light theme\nUpdate About page\nFooter\nHow to execute python code and show figs?\nCenter Fig Caption"
  },
  {
    "objectID": "posts/RoPE/index.html",
    "href": "posts/RoPE/index.html",
    "title": "RoPE clearly explained",
    "section": "",
    "text": "Introduction\nThere are plenty of good resources explaining the transformer architecture online, but Rotary Position Embedding (RoPE) is often poorly explained or skipped entirely.\nRoPE was first introduced in this paper, and while the mathematical operations involved are relatively straightforward – primarily rotation matrix and matrix multiplications – the real challenge lies in understanding the intuition behind how it works. I’ll try to provide a way to visualize what it’s doing to vectors and explain why this approach is so effective.\nI assume you have a basic understanding of transformers and the attention mechanism throughout this post.\n\n\nRoPE Intuition\nSince transformers lack inherent understanding of order and distances, researchers developed positional embeddings. Here’s what positional embeddings should accomplish:\n\nTokens closer to each other should attend with higher weights, while distant tokens should attend with lower weights.\nPosition within a sequence shouldn’t matter – if two words are close to each other, they should attend to each other with higher weights regardless of whether they appear at the beginning or end of a long sequence.\nTo accomplish these goals, relative positional embeddings are far more useful than absolute position embeddings.\n\nKey insight: LLMs should focus on relative positions, which is what truly matters.\nIf you understand these concepts, you’re already halfway there.\n\n\nBefore RoPE\nThe original positional embeddings from Attention is All You Need used predefined values (defined by a formula) added to semantic embeddings. While the formula made sense, and RoPE does something similar, mixing semantics and position was confusing-not just for humans, but for LLMs too. Later research confirmed that LLMs were memorizing (overfitting) rather than generalizing positions, causing rapid deterioration when sequence lengths exceeded training data.\nOne strategy that proved successful in early deep learning was: when unsure how to compute useful features for a neural network, let the network learn them itself! That’s what models like GPT-3 did – they learned their own position embeddings. However, providing too much freedom increases overfitting risks and, in this case, creates hard limits on context windows.\nThe best approaches focused on modifying the attention mechanism so that nearby tokens receive higher attention weights while distant tokens receive lower weights. The goal was to cleverly modify \\(Q\\) and \\(K\\) so their dot products would reflect proximity while preserving hidden states.\n\n\nRotation Intuition\nRoPE modifies \\(Q\\) and \\(K\\) by applying rotations to them.\nLet \\(q\\) be the query projection of a token and \\(k\\) be the key projection of another. For tokens that are close in the text, minimal rotation is applied, while distant tokens undergo larger rotational transformations.\nImagine two identical projection vectors – any rotation would make them more distant from each other. That’s exactly what we want.\n\n\n\nRoPE Rotation Animation\n\n\nNow, here’s a potentially confusing situation: if two projection vectors are already far apart, rotation might bring them closer together. That’s not what we want! They’re being rotated because they’re distant in the text, so they shouldn’t receive high attention weights. Why does this still work?\n\nIn 2D, there’s only one rotation plane (\\(xy\\)). You can only rotate clockwise or counterclockwise.\nIn 3D, there are infinitely many rotation planes, making it highly unlikely that rotation will bring two vectors closer together.\nModern models operate in very high-dimensional spaces (10k+ dimensions), making this even more improbable.\n\n\n\nRemember: in deep learning, probabilities matter most! It’s acceptable to be occasionally wrong as long as the probabilities are low.\n\n\nAngle of Rotation\nThe rotation angle depends on two factors: \\(m\\) and \\(i\\). Let’s examine each.\n\nToken Absolute Position \\(m\\)\nRotation increases as the token’s absolute position \\(m\\) increases.\nI know what you’re thinking: “\\(m\\) is absolute position, but didn’t you say relative positions matter most?”\nHere’s the magic: consider a 2D plane where you rotate one vector by \\(\\alpha\\) and another by \\(\\beta\\). The angular difference between them becomes \\(\\alpha - \\beta\\). Their absolute values don’t matter – only the distance does. So for two tokens at positions \\(m\\) and \\(n\\), the rotation modifies the angle between them proportionally to \\(m-n\\).\n\n\n\nRelative distance after rotation\n\n\n\nFor simplicity, let’s assume we’re only rotating \\(q\\) (this is mathematically accurate since we care about final distances, not coordinates).\n\n\n\nHidden State Index \\(i\\)\nInstead of applying uniform rotation across all hidden state dimensions, RoPE processes two dimensions at a time, applying different rotation angles to each pair. In other words, it breaks the long vector into multiple pairs that can be rotated in 2D by different angles.\nWe rotate hidden state dimensions differently – rotation is higher when \\(i\\) is low (vector beginning) and lower when \\(i\\) is high (vector end).\nUnderstanding this operation is straightforward; understanding why we need it requires more explanation:\n\nIt allows the model to choose what should have shorter or longer ranges of influence.\n\nImagine vectors in 3D (\\(xyz\\)).\nThe \\(x\\) and \\(y\\) axes represent early dimensions (low \\(i\\)) that undergo higher rotation. Tokens projected mainly onto \\(x\\) and \\(y\\) must be very close to attend with high intensity.\nThe \\(z\\) axis, where \\(i\\) is higher, rotates less. Tokens projected mainly onto \\(z\\) can attend even when distant.\n\n\n\n\n\nWe apply rotation on the \\(xy\\) plane. Two vectors encoding information mainly in \\(z\\) remain close despite rotation (tokens that should attend despite longer distances!)\n\n\n\n\n\nTwo vectors encoding information in \\(x\\) and \\(y\\) become very far apart (nearby tokens where one shouldn’t attend to the other).\n\n\nThis structure captures complicated nuances in human language – pretty cool, right?\nOnce again, I know what you’re thinking: “after too much rotation, they start getting close again”.\nThat’s correct, but here’s why it still works:\n\nWe’re visualizing in 3D, but this actually happens in much higher dimensions.\nAlthough some dimensions grow closer, others that rotate more slowly continue growing farther apart. Hence the importance of rotating dimensions by different angles.\nRoPE isn’t perfect – due to its rotational nature, local maxima do occur. See the theoretical chart from the original authors:\n\n\n\n\nTheoretical curve provided by the authors Source: ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING\n\n\nThe theoretical curve has some crazy bumps, but in practice I found it to be much more behaved:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef rope_angle_between(q, k, d, base=10000.0):\n    q = np.asarray(q, dtype=np.float64)\n    k = np.asarray(k, dtype=np.float64)\n\n    D = len(k)\n    i = np.arange(0, D // 2, dtype=np.float64)\n    thetas = np.power(base, -2.0 * i / D)\n\n    angles = d * thetas\n    cos_t = np.cos(angles)\n    sin_t = np.sin(angles)\n\n    q_even = q[0::2]\n    q_odd = q[1::2]\n\n    q_rot_even = cos_t * q_even - sin_t * q_odd\n    q_rot_odd = sin_t * q_even + cos_t * q_odd\n\n    b_rot = np.empty_like(q)\n    b_rot[0::2] = q_rot_even\n    b_rot[1::2] = q_rot_odd\n\n    dot = float(np.dot(k, b_rot))\n    na = float(np.linalg.norm(k))\n    nb = float(np.linalg.norm(b_rot))\n    cosine = dot / (na * nb)\n    return cosine\n\n\nrng = np.random.default_rng(42)\nD = 10_000  # even dimension\nvec_a = rng.normal(size=D)\nvec_b = vec_a  # initially equal (very close)\n\nmax_d = 500\nds = np.arange(max_d + 1)\nangles = np.array([rope_angle_between(vec_a, vec_b, int(d)) for d in ds])\n\nplt.figure(figsize=(8, 4.5))\nplt.plot(ds, angles, lw=1.2)\nplt.xlabel(\"Relative distance\")\nplt.ylabel(\"Cosine similarity\")\nplt.title(\"Cosine similarity between RoPE-rotated vectors vs relative distance\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSome ideas that occurred to me: clipping the rotation angle so the similarity is a strictly decreasing function (or even picking something other than sin/cos). I wonder if newer papers have explored this approach. I’ve seen clipping being applied to other techniques, but not to RoPE. Anyways, let’s keep moving forward…\nBare in mind that cosine similarity tends to grow (although slowly) as the distance grows a lot past our base (10,000 in this case). A simple solution here is to increase the base, or even let techniques like local attention take care of it.\n\nBottom line: The LLM learns to encode long-range and short-range meaning influences in different dimensions of the hidden state.\n\nHere are concrete examples:\n\nThe LLM processes Python code where an initial transformation is applied to dataframe df. This relevant information should potentially carry over a long range.\nAdjectives typically characterize nearby nouns. In “A beautiful mountain stretches beyond the valley”, the adjective beautiful specifically describes the mountain, not the valley, so it should primarily affect the mountain embedding.\n\n\n\n\nThe Angle Formula\nNow that you understand the concepts and have strong intuition, here are the equations. The rotation angle is defined by \\(m \\times \\theta\\), where \\(m\\) is the token’s absolute position and \\(\\theta = 10000^{-2(i-1)/d_{model}}\\), with \\(i \\in \\{1, 2, \\ldots, d/2\\}\\) representing hidden state dimensions.\nWhen \\(i=1 \\Rightarrow \\theta=1\\) (high) and when \\(i=d/2 \\Rightarrow \\theta \\approx 1/10000\\) (low).\n\n\nConclusion\n\nWe should find clever ways to inject knowledge into LLMs rather than letting them learn everything independently.\n\nWe do this by providing the right operations a neural network needs to process data – attention and convolutions are great examples.\n\nClosed-form equations can extend indefinitely since you don’t need to learn each position embedding.\n\nThis is why RoPE provides excellent sequence length flexibility.\n\nThe most important property: attention weights decrease as relative distances increase.\n\nThis follows the same intuition as local attention in alternating attention architectures."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Blog",
    "section": "",
    "text": "Here’s where I document and share some of my learnings in Machine Learning and Artificial Intelligence :)\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nRoPE clearly explained\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  }
]